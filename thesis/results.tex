\chapter{Results}
\label{ch:results}

In this chapter, an analysis is conducted of the results obtained from the execution of multiple tests on the software under investigation. Since tests over the implementation's correctness are trivial by design the focus is on network speed. The results are then compared with the performance of OT systems traditional networking techniques. Furthermore, an assessment is made of the advantages and disadvantages of the L7 switch. 

\section{Rust/AF\_XDP filtered-unicast traffic}
In order to ascertain the viability of our implementation as a substitute for current standards, it was necessary to establish a baseline, defined as the average speed at which UDP packets travel in a maritime network. Due to the unavailability of a real OT environment for testing our switch, all tests were conducted using the previously analyzed Network Namespace technology. 

Given the information flow characteristics of naval systems, as outlined in the background section, the initial tests centered on the normal unfiltered data exchange. As the data payload values were not a factor in this testing phase and the generation of a plausible amount of data was necessary, the iperf3 tool was selected. This software enabled easily the recreation of a reliable environment, in which data flows at approximately 1 Gbit/s with a default UDP packet size of 1460 KB between two components.

Initially, it was necessary to ascertain whether the implementation of system emulation utilizing network namespaces would result in the introduction of overhead, which could potentially compromise the reliability of the tests from the outset. Consequently, experiments were executed on the data exchange between emulated components, leveraging the Linux Network Stack with base parameters.
Subsequent to the initial testing, additional trials were executed in a second iteration, employing maximum values to assess the limits of data exchange.

\leavevmode\newline\\

\begin{tabularx}{\textwidth}{|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|}
  \hline
  \multicolumn{3}{|c|}{\textbf{Iperf tool results - Linux Network Stack Unicast }} \\ \hline
    Role & Bitrate Mbit/s & Datagram size B\\ \hline
    SENDER & 1000& 1460\\ \hline
    RECEIVER  & 1000 & 1460    \\ \hline
    SENDER & 4800 & 8972\\ \hline
    RECEIVER  & 4800  & 8972 \\ \hline
\end{tabularx}

\leavevmode\newline\\

As evidenced by the iperf tool's output, the namespace abstraction did not result in any additional overhead maintaining a bitrate of 1 Gbit/s between the sender and receiver. Having established a baseline that should emulate a real-life scenario, we proceeded to test a filtered-unicast information flow, leveraging our switch implementation with the same iperf parameters.

\leavevmode\newline\\

\begin{tabularx}{\textwidth}{|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|}
  \hline
  \multicolumn{3}{|c|}{\textbf{Iperf tool results - Rust/AF\_XDP Unicast }} \\ \hline
    Role & Bitrate Mbit/s & Datagram size B\\ \hline
    SENDER & 1000& 1460\\ \hline
    RECEIVER  & 1000 & 1460    \\ \hline
    SENDER & 4800 & 8972\\ \hline
    RECEIVER  & 4800  & 8972 \\ \hline
\end{tabularx}


\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_sender.png}
	\caption{Bitrate over time - Sender AF\_XDP- Unicast flow - 1460B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_sender}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_receiver.png}
	\caption{Bitrate over time - Receiver Rust/AF\_XDP- Unicast flow - 1460B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_receiver}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_sender_5.png}
	\caption{Bitrate over time - Sender Rust/AF\_XDP- Unicast flow - 8972B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_sender_5}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_receiver_5.png}
	\caption{Bitrate over time - Receiver AF\_XDP- Unicast flow - 8972B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_receiver_5}
\end{figure}

An examination of the iperf results and the graphs obtained from analyzing the network traffic within the switch on a twenty seconds simulation revealed that our implementation did not modify the data exchange speed over time. This observation indicates that the Linux Network stack baseline was maintained without introducing additional overhead in the filtered-unicast scenario with a base traffic bitrate of 1Gbit/s with a dataframe size of 1460B and a maximum traffic bitrate of 4.8Gbit/s with a dataframe size of 8972B.


\section{Rust/AF\_XDP filtered-multicast traffic}
In pursuit of the implementation's objective to engineer a tool that would demonstrate seamless integration with an existing network while maintaining complete architecture-agnosticism, the multicast traffic feature was implemented through a process of packet multiplication operating at the application layer, thereby circumventing reliance on addresses. This strategy aligns with the principles of Software Defined Networking, fostering modularity within the network itself by defining system components' relationships dynamically through a policy file, eliminating the need for modifications to the network architecture.

In accordance with the established protocol, packets are multicasted to the policy-defined components subsequent to the validation of their payload. To this end, a dedicated tool was developed to enable the transmission of custom dataframes that adhere to the protocol's structure, thereby circumventing the utilization of randomly generated packets by iperf.To emulate a realistic scenario, three components were selected: Girobussola, ECDIS, and Radar. According to NMEA protocol's rules whenever the "girobussola" sends a message, containing the talker ID value "II" and the sentence type value "HDT", this should be multicasted to the ECDIS and Radar components. In this scenario, the traffic was emulated using the same parameters that had been set previously in iPerf. The results obtained were as follows:

\leavevmode\newline\\

\begin{tabularx}{\textwidth}{|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|}
  \hline
  \multicolumn{3}{|c|}{\textbf{Iperf tool results - Rust/AF\_XDP Multicast}} \\ \hline
    Role & Bitrate Mbit/s & Datagram size B\\ \hline
    SENDER\_Girobussola & 1120 & 1460\\ \hline
    RECEIVER\_ECDIS  & 1120 & 1460    \\ \hline
    RECEIVER\_Radar & 1120 & 1460 \\ \hline
    SENDER\_Girobussola & 5100 & 8972\\ \hline
    RECEIVER\_ECDIS  & 800  & 8972 \\ \hline
    RECEIVER\_Radar & 5100 & 8972 \\ \hline
\end{tabularx}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_multicast_sender.png}
	\caption{Bitrate over time - Sender\_Girobussola AF\_XDP - Multicast flow - 1460B Dataframe size}
    \label{fig:rust_performance_AF_XDP_sender_multicast}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_multicast_receiver_radar.png}
	\caption{Bitrate over time - Receiver\_Radar Rust/AF\_XDP - Multicast flow - 1460B Dataframe size}
    \label{fig:rust_performance_AF_XDP_receiver_radar}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_multicast_receiver_ECDIS.png}
	\caption{Bitrate over time - Receiver\_ECDIS Rust/AF\_XDP - Multicast flow - 1460B Dataframe size}
    \label{fig:rust_performance_AF_XDP_receiver_ecdis}
\end{figure}


\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_multicast_sender_5.png}
	\caption{Bitrate over time - Sender\_Girobussola AF\_XDP - Multicast flow - 8972B Dataframe size}
    \label{fig:rust_performance_AF_XDP_sender_multicast}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_multicast_receiver_radar_5.png}
	\caption{Bitrate over time - Receiver\_Radar Rust/AF\_XDP - Multicast flow - 8972B Dataframe size}
    \label{fig:rust_performance_AF_XDP_receiver_radar}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_multicast_receiver_ecdis_5.png}
	\caption{Bitrate over time - Receiver\_ECDIS Rust/AF\_XDP - Multicast flow - 8972B Dataframe size}
    \label{fig:rust_performance_AF_XDP_receiver_ecdis}
\end{figure}

\leavevmode\newline

The graphs above illustrate the switch performances while working with different datagram sizes, 1460B and 8972B.
The results coming from the ECDIS receiver component working with heavier packets are of particular interest. While performances reached working with lighter datagrams demonstrate an identical trend to the ones in the unicast traffic simulation, an anomaly is observed for more demanding workloads. This phenomenon can be explained by examining our implementation of multicast functionality. Rather than relying on multicast addresses, packets must be copied completely and then modified to be multicasted. Doing so the performance remains unaltered when datagrams are relatively small while adding computational load to the switch results in a decline in performance as datagram size increases.

The ECDIS component received packets at a lower bitrate because it was  not the primary target of the sender. Given that the primary target was the radar component, there was no need to copy and modify packets for transmission to it. Conversely, the ECDIS component was a secondary target, and the multicast process operations resulted in a decline in performance when handling more substantial packets. This phenomenon is made evident with maximum datagram lengths. This represents the sole constraint in our implementation, wherein we prioritize architectural flexibility over performance for non-standard network workloads.