\chapter{Results}
\label{ch:results}

In this chapter, an analysis is conducted of the results obtained from the execution of multiple tests on the software under investigation. Since tests over the implementation's correctness are trivial by design the focus is on network speed. The results are then compared with the performance of OT systems traditional networking techniques. Furthermore, an assessment is made of the advantages and disadvantages of the L7 switch. 

\section{Rust/AF\_XDP filtered-unicast traffic}
In order to ascertain the viability of our implementation as a substitute for current standards, it was necessary to establish a baseline, defined as the average speed at which UDP packets travel in a maritime network. Due to the unavailability of a real OT environment for testing our switch, all tests were conducted using the previously analyzed Network Namespace technology. 

Given the information flow characteristics of naval systems, as outlined in the background section, the initial tests centered on the normal unfiltered data exchange. As the data payload values were not a factor in this testing phase and the generation of a plausible amount of data was necessary, the iperf3 tool was selected. This software enabled easily the recreation of a reliable environment, in which data flows at approximately 1 Gbit/s with a default UDP packet size of 1460 KB between two components.

Initially, it was necessary to ascertain whether the implementation of system emulation utilizing network namespaces would result in the introduction of overhead, which could potentially compromise the reliability of the tests from the outset. Consequently, experiments were executed on the data exchange between emulated components, leveraging the Linux Network Stack with base parameters.
Subsequent to the initial testing, additional trials were executed in a second iteration, employing maximum values to assess the limits of data exchange.

\leavevmode\newline\\

\begin{tabularx}{\textwidth}{|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|}
  \hline
  \multicolumn{3}{|c|}{\textbf{Iperf tool results - Linux Network Stack Unicast }} \\ \hline
    Role & Bitrate Mbit/s & Datagram size B\\ \hline
    SENDER & 1000& 1460\\ \hline
    RECEIVER  & 1000 & 1460    \\ \hline
    SENDER & 4800 & 8972\\ \hline
    RECEIVER  & 4800  & 8972 \\ \hline
\end{tabularx}

\leavevmode\newline\\

As evidenced by the iperf tool's output, the namespace abstraction did not result in any additional overhead maintaining a bitrate of 1 Gbit/s between the sender and receiver. Having established a baseline that should emulate a real-life scenario, we proceeded to test a filtered-unicast information flow, leveraging our switch implementation with the same iperf parameters.

\leavevmode\newline\\

\begin{tabularx}{\textwidth}{|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|>{\hsize=1\hsize}X|}
  \hline
  \multicolumn{3}{|c|}{\textbf{Iperf tool results - Rust/AF\_XDP Unicast }} \\ \hline
    Role & Bitrate Mbit/s & Datagram size B\\ \hline
    SENDER & 1000& 1460\\ \hline
    RECEIVER  & 1000 & 1460    \\ \hline
    SENDER & 4800 & 8972\\ \hline
    RECEIVER  & 4800  & 8972 \\ \hline
\end{tabularx}


\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_sender.png}
	\caption{Bitrate over time - Sender AF\_XDP- Unicast flow - 1460B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_sender}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_receiver.png}
	\caption{Bitrate over time - Receiver Rust/AF\_XDP- Unicast flow - 1460B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_receiver}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_sender_5.png}
	\caption{Bitrate over time - Sender Rust/AF\_XDP- Unicast flow - 8972B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_sender_5}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[scale=0.45]{thesis/images/rust_performance_afxdp_unicast_receiver_5.png}
	\caption{Bitrate over time - Receiver AF\_XDP- Unicast flow - 8972B Dataframe size}
    \label{fig:iperf_performance_AF_XDP_receiver_5}
\end{figure}

An examination of the iperf results and the graphs obtained from analyzing the network traffic within the switch revealed that our implementation did not modify the data exchange speed over time. This observation indicates that the Linux Network stack baseline was maintained without introducing additional overhead in the filtered-unicast scenario.


\section{Rust/AF\_XDP filtered-multicast traffic}
In pursuit of the implementation's objective to engineer a tool that would demonstrate seamless integration with an existing network while maintaining complete architecture-agnosticism, the multicast-traffic feature was implemented through a process of packet multiplication operating at the application layer, thereby circumventing reliance on addresses. This strategy aligns with the principles of Software Defined Networking, fostering modularity within the network itself by defining system components' relationships dynamically through a policy file, eliminating the need for modifications to the network architecture.
Coherently since packets get multicasted to the policy-defined components once their payload is accepted as valid NMEA and verified we had to develop a tool that let us send custom datagrams that respected the protocol's structure instead of relying on "garbage" filled packets generated by iperf.



